{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Titanic Competition Introduction:**\n\nOur problem is to predict which passengers survived the Titanic's maiden voyage given the following information about them:\n\n- *Age* in years old\n- *Sex* $\\in$ {female, male}\n- *SibSp* representing the number of siblings and spouses travelling with the passenger\n- *Parch* representing the number of parents and children travelling with the passenger\n- *Pclass* representing the passenger's ticket class $\\in$ {1st class, 2nd class, 3rd class}\n- *Ticket* number\n- *Cabin* number\n- *Fare* paid for ticket\n- *Embarked* representing the port where the passenger embarked $\\in$ {C = Cherbourg, Q = Queenstown, S = Southampton}\n- *Survived* $\\in$ {0 = died, 1 = survived} for the training set, target for test \n\nThis represents a binary classification problem as the passengers either survived, or they did not.","metadata":{}},{"cell_type":"code","source":"# Data analysis\nimport numpy as np\nimport pandas as pd\npd.set_option(\"display.precision\", 4)\nfrom math import isnan\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_absolute_error\n\n# Peace of mind\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Define constants\nTRAIN_LEN = 891\nRNG_SEED = 343\nCOLS_TO_DROP = []\nCHILD_AGE_END = 18\nDECKS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'N']\nDEFAULT_SURVIVAL = 0.5\n\n##########################################\n########## Function definitions ##########\n##########################################\ndef get_deck_class_count(df, T_deck):\n    \"\"\"\n    get_deck_class_count reformats counts of Titanic passengers for deck, class\n    combinations into a dictionary along with providing the percentages/\n    proportion of each class on each deck\n\n    :param df: dataframe grouped by Deck and Pclass storing the count for \n        each combination\n    :param T_deck: boolean that indicates if we are including 'T' deck or not\n    :return: two dictionaries with passenger class counts and proportions for \n        each deck\n    \"\"\" \n    # Dictionaries storing passenger counts and proportions for all deck, \n    # class combinations\n    deck_count = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {},\n                  'G': {}, 'N': {}, 'T': {}}\n    if not T_deck:\n        deck_count.pop('T', None)\n    deck_percent = {}\n    decks = df.transpose().columns.levels[0] # List of decks\n    \n    for deck in decks:\n        # Populate deck_count\n        for pclass in range(1, 4):\n            try:\n                count = int(df.loc[deck, pclass])\n                deck_count[deck][pclass] = count\n            except KeyError:\n                deck_count[deck][pclass] = 0\n                \n        # Populate deck_percent\n        deck_percent[deck] = [(count / sum(deck_count[deck].values())) \n                              * 100 for count in deck_count[deck].values()]\n        \n    return deck_count, deck_percent    \n\n\ndef get_surv_prop(deck, pclass):\n    \"\"\"\n    surv_prop provides the percentage survival for passengers given their deck\n    and pclass\n\n    :param deck: string denoting the deck of a passenger\n    :param pclass: integer denoting the pclass of a passenger\n    :return: decimal percentage of survival for similar passengers\n    \"\"\" \n    \n    return deck_class_surv_prop[deck][pclass]\n\n\ndef get_corr(df):\n    \"\"\"\n    get_corr calculates and returns a dataframe of correlations for an input\n    dataframe's features\n    \n    :param df: dataframe containing features we want correlations of\n    :return: dataframe of correlations excluding duplicate and self correlations\n    \"\"\"\n    # Calculate correlation and sort output\n    corr = df.corr().abs().unstack().sort_values(kind=\"quicksort\", \n                                                 ascending=False).reset_index()\n    \n    # Remap correlation and drop duplicate/self correlations\n    cols_map = {\"level_0\": \"Feature 1\", \n                \"level_1\": \"Feature 2\", \n                0: 'Correlation Coefficient'}\n    \n    # Every 2nd value is the reverse of the previous\n    corr.drop(corr.iloc[1::2].index, inplace=True)\n    corr.rename(columns=cols_map, inplace=True)\n    \n    return corr.drop(corr[corr['Correlation Coefficient'] == 1.0].index)\n\n\ndef combine_df(df1, df2):\n    \"\"\"\n    combine_df combines df1 and df2 into a new dataframe assuming same columns\n    \n    :param df1: dataframe that will be at the start/top of resultant dataframe\n    :param df2: dataframe that will be at the end/bottom of resultant dataframe\n    :return: combined dataframe composed of df1 and df2\n    \"\"\"\n    \n    return pd.concat([df1, df2], sort=True).reset_index(drop=True)\n\n\ndef divide_df(df, first_len):\n    \"\"\"\n    divide_df divides df into two parts, splitting after first_len rows of df and\n    dropping 'Survived' column from the second dataframe\n    \n    :param df: dataframe desired to be split\n    :param first_len: integer representing the number of rows in the first dataframe\n    :return: two dataframes split from df\n    \"\"\"\n    \n    # Slicing with loc includes start AND end\n    return df.loc[:first_len - 1], df.loc[first_len:].drop(['Survived'], axis=1)\n\n\ndef drop_cols(cols):\n    \"\"\"\n    drop_cols drops col from both the train and test data sets\n    \n    :param cols: list of strings that signifies which column(s) to drop\n    :return: None, columns dropped in place\n    \"\"\"\n    for col in cols:\n        train.drop([col], inplace=True, axis=1)\n        test.drop([col], inplace=True, axis=1)\n    \n    return\n\n\ndef display_class_dist(percentages, y_label, title):\n    \"\"\"\n    display_class_dist displays the distribution of classes among the \n    decks of the Titanic\n\n    :param percentages: dictionary with passenger class proportions by deck, \n        returned by get_deck_class_count\n    :param y_label: string label for y-axis of plot\n    :param title: string title for plot\n    :return: None\n    \"\"\"\n    df_percent = pd.DataFrame(percentages).transpose()\n    deck_names = percentages.keys()\n    bar_count = np.arange(len(deck_names))\n    bar_width = 0.75\n    \n    # Plot data\n    plt.figure(figsize=(16, 8))\n    plt.bar(bar_count, df_percent[0], \n            color='red', edgecolor='black', width=bar_width, \n            label='Passenger Class 1')\n    plt.bar(bar_count, df_percent[1], bottom=df_percent[0], \n            color='lime', edgecolor='black', width=bar_width, \n            label='Passenger Class 2')\n    plt.bar(bar_count, df_percent[2], bottom=df_percent[0] + df_percent[1], \n            color='blue', edgecolor='black', width=bar_width, \n            label='Passenger Class 3')\n    \n    # Tune plot\n    plt.xlabel('Deck', size=25)\n    plt.ylabel(y_label, size=25)\n    plt.xticks(bar_count, deck_names)\n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    plt.legend(loc='upper right', prop={'size': 15})\n    plt.title(title, size=30, y=1)   \n    plt.show()\n    \n    return\n\n\ndef group_survivors(df, group, new_feature_name):\n    \"\"\"\n    group_survivors calculates and creates a new feature denoting whether\n    passengers in their given group are known to have survived or not using\n    placeholder values in the new feature\n    \n    :param df: dataframe which has at least the same columns as the default \n        titanic dataframes\n    :param group: list of strings denoting the features to be used for grouping\n    :param new_feature_name: string of the new feature to be calculated\n    :return: updated version of df with the new feature included\n    \"\"\"\n    df[new_feature_name] = DEFAULT_SURVIVAL\n    \n    for _, group_df in df.groupby(group):\n        if len(group_df) > 1:\n            # Store whether any group members are known to have survived/died\n            surv_max = group_df['Survived'].max()\n            surv_min = group_df['Survived'].min()\n\n            # Skip if no known survivors/deaths in training set\n            if isnan(surv_max) and isnan(surv_min):\n                continue\n\n            # Assign non-default values to the new feature where\n            # a member is known to have survived/died\n            for _, row in group_df.iterrows():\n                passId = row['PassengerId']\n                if (surv_max == 1.0):\n                    df.loc[df['PassengerId'] == passId, new_feature_name] = 1.0\n                elif (surv_min==0.0):\n                    df.loc[df['PassengerId'] == passId, new_feature_name] = 0.0\n                    \n    return df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-11T19:29:52.845959Z","iopub.execute_input":"2021-12-11T19:29:52.846302Z","iopub.status.idle":"2021-12-11T19:29:52.87383Z","shell.execute_reply.started":"2021-12-11T19:29:52.846264Z","shell.execute_reply":"2021-12-11T19:29:52.872752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the data\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\ncombined = combine_df(train, test)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:52.875724Z","iopub.execute_input":"2021-12-11T19:29:52.876031Z","iopub.status.idle":"2021-12-11T19:29:52.908837Z","shell.execute_reply.started":"2021-12-11T19:29:52.876003Z","shell.execute_reply":"2021-12-11T19:29:52.908165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Understand the Dataset:**\n\nTo begin, we load in the data and then familiarize ourselves with the coarse grain aspects like datatypes, null values, etc.\n\nNumerical:\n\n- Training set has 891 samples\n- Testing set has 418 samples\n- Both *Age* and *Fare* features have distributions biased towards lower values with <1% high outliers\n- 76% of passengers did not travel with parents or children\n- 30% of the passengers had siblings and/or spouse aboard\n- 38% of passengers survived in the training set compared to the actual survival rate of 32%\n\nThese can be seen by modifying *percentiles* in the following cell.\n\nCategorical:\n\n- 77% of *Cabin* values are missing, and of the ones that exist, 37% are duplicates\n- There are 3 places to embark from, with 70% embarking from S=Southampton\n- All but two values for *Name* are unique\n- Male and Female options for *Sex*, with 64% of passengers being male\n- 29% of *Ticket* values are duplicates and there is at least one ticket that shows up 11 times","metadata":{}},{"cell_type":"code","source":"# Review 'Parch' distribution with 'percentiles=[.76, .77]'\n# Review 'SibSp' distribution with 'percentiles=[.68, .69]'\n# Review 'Age' and 'Fare' distribution with 'percentiles=[.9, .99]'\n# Review 'Survived' distribution with 'percentiles=[.61, .62]'\npercentiles = [.25, .50, .75]\n\n# View the data\ndisplay(combined.head())\ndisplay(combined.describe(percentiles=percentiles))\ndisplay(combined.describe(include=['O']))\ndisplay(combined.info())\n\n# Remove rows with missing target\ntrain.dropna(axis=0, subset=['Survived'], inplace=True)\n\n# Check which columns have null values and print\nprint('Number of null values by feature in combined train and test set:', \n      combined.isnull().sum()[combined.isnull().sum()>0], sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:52.910394Z","iopub.execute_input":"2021-12-11T19:29:52.91073Z","iopub.status.idle":"2021-12-11T19:29:52.994887Z","shell.execute_reply.started":"2021-12-11T19:29:52.91067Z","shell.execute_reply":"2021-12-11T19:29:52.99376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Investigate Assumptions:**\n\nIt is worth noting some assumptions based on the information we have.  We know that women and children had priority when evacuating.  Consequently, we expect women to be overrepresented in *Survived*.  We also know that class always comes into play when resources are limited (especially since first class accomodations were closest to lifeboats).  By the same logic, we expect passengers in class 1 will be more likely to survive than their class 2 and 3 counterparts.  We will test these assumptions noting that we have not yet filled in all *Age* values, so our results will not be completely accurate.","metadata":{}},{"cell_type":"code","source":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.005561Z","iopub.execute_input":"2021-12-11T19:29:53.006672Z","iopub.status.idle":"2021-12-11T19:29:53.021405Z","shell.execute_reply.started":"2021-12-11T19:29:53.006632Z","shell.execute_reply":"2021-12-11T19:29:53.020623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see above that there is a very significant disparity in survival rates between men and women.  Next we can check survival rate differences between adults and children.","metadata":{}},{"cell_type":"code","source":"adults = train[train['Age'] >= CHILD_AGE_END]\nchildren = train[train['Age'] < CHILD_AGE_END]\n\nprint('Proportion of passengers <{} who survived: {:.4f}'.format(CHILD_AGE_END, children['Survived'].mean()))\nprint('Proportion of passengers >={} who survived: {:.4f}'.format(CHILD_AGE_END, adults['Survived'].mean()))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.02277Z","iopub.execute_input":"2021-12-11T19:29:53.023546Z","iopub.status.idle":"2021-12-11T19:29:53.034783Z","shell.execute_reply.started":"2021-12-11T19:29:53.0235Z","shell.execute_reply":"2021-12-11T19:29:53.034008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is not as large of a disparity in this case, but it is still significant.  Finally we take a look at the survival rates by passenger class.","metadata":{}},{"cell_type":"code","source":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.035775Z","iopub.execute_input":"2021-12-11T19:29:53.03669Z","iopub.status.idle":"2021-12-11T19:29:53.055342Z","shell.execute_reply.started":"2021-12-11T19:29:53.036656Z","shell.execute_reply":"2021-12-11T19:29:53.054406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we see that there is a significant difference in survival rates based on this feature.  Our assumptions were well-founded.  We will proceed with these results in mind as we process the data and make predictions.","metadata":{}},{"cell_type":"markdown","source":"**Filling Fare Null Values:**\n\nThere is only one null value for *Fare*.  *Pclass* is certainly relevant for *Fare*.  It seems like having no family members travelling with them would also affect *Fare* (*Parch* = 0 and *SibSp* = 0).  Knowing this, we can fill in the median value of similar passengers.","metadata":{}},{"cell_type":"code","source":"combined[combined['Fare'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.056661Z","iopub.execute_input":"2021-12-11T19:29:53.057232Z","iopub.status.idle":"2021-12-11T19:29:53.07515Z","shell.execute_reply.started":"2021-12-11T19:29:53.057193Z","shell.execute_reply":"2021-12-11T19:29:53.074525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see below that the median fare increases not only as the class of the ticket increases, but also as the number of family members travelling with the individual increases (there are exceptions, but the trend is there).","metadata":{}},{"cell_type":"code","source":"# Calculate median fares by class and number of family members who are also passengers (training data)\nmed_fare = train.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()\nprint('Median fares for passengers by Pclass, Parch, and SibSp combinations:', \n      med_fare, sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.077304Z","iopub.execute_input":"2021-12-11T19:29:53.077658Z","iopub.status.idle":"2021-12-11T19:29:53.087379Z","shell.execute_reply.started":"2021-12-11T19:29:53.077617Z","shell.execute_reply":"2021-12-11T19:29:53.086525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seeing this trend, we can now confidently fill our null values using our proposed method.  We ensure that the calculation of parameters is done exclusively using the training set to avoid leakage.","metadata":{}},{"cell_type":"code","source":"# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\ncombined['Fare'] = combined['Fare'].fillna(med_fare[3][0][0])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.088854Z","iopub.execute_input":"2021-12-11T19:29:53.089334Z","iopub.status.idle":"2021-12-11T19:29:53.099723Z","shell.execute_reply.started":"2021-12-11T19:29:53.089294Z","shell.execute_reply":"2021-12-11T19:29:53.099012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Filling Embarked Null Values:**\n\nThe only null values for *Embarked* are from two first class women who have the same ticket number.  From this, we will proceed assuming that they boarded the ship from the same port (one of C = Cherbourg, Q = Queenstown, or S = Southampton).","metadata":{}},{"cell_type":"code","source":"combined[combined['Embarked'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.101033Z","iopub.execute_input":"2021-12-11T19:29:53.101521Z","iopub.status.idle":"2021-12-11T19:29:53.123851Z","shell.execute_reply.started":"2021-12-11T19:29:53.101481Z","shell.execute_reply":"2021-12-11T19:29:53.122651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the story of the Titanic and its sinking is so well researched, just Googling their names reveal the true value of our missing values.  [Encylopedia Titanica](https://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html) tells us that\n\n> Mrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28.\n\nWith this, we fill in the Embarked missing values with S.","metadata":{}},{"cell_type":"code","source":"combined['Embarked'] = combined['Embarked'].fillna('S')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.125378Z","iopub.execute_input":"2021-12-11T19:29:53.125862Z","iopub.status.idle":"2021-12-11T19:29:53.131514Z","shell.execute_reply.started":"2021-12-11T19:29:53.125819Z","shell.execute_reply":"2021-12-11T19:29:53.130628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Filling Age Null Values:**\n\nWe first check to see if *Age* is correlated with other numeric features.   If it is correlated with other features, we can fill in our missing values more appropriately than a simple global average.","metadata":{}},{"cell_type":"code","source":"# Get pairwise correlations of 'Age' feature with the other numeric features\ntrain, test = divide_df(combined, TRAIN_LEN)\ncorr_train = get_corr(train)\ncorr_train[(corr_train['Feature 1'] == 'Age') | \n           (corr_train['Feature 2'] == 'Age')]","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.132781Z","iopub.execute_input":"2021-12-11T19:29:53.133582Z","iopub.status.idle":"2021-12-11T19:29:53.15827Z","shell.execute_reply.started":"2021-12-11T19:29:53.133537Z","shell.execute_reply":"2021-12-11T19:29:53.157495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that *Age* is correlated with *Pclass*.  *Sex* together with *Pclass* provides yet better predictions seen by the varied medians below.  We see two distinct trends, women having a lower median age than men for a given class, as well as median age increasing as class increases (class 1 is 'higher' than class 3 ie. more expensive).  As a result, it is necessary to take these relations into account when filling null values.","metadata":{}},{"cell_type":"code","source":"# Calculate median ages for each Pclass and sex combinations (training data)\nage_pclass_sex = train.groupby(['Pclass', 'Sex']).median()['Age']\n\n# Print data for easier viewing\nprint('Median ages for the following groups (training data):')\nfor pclass in range(1, train['Pclass'].nunique() + 1):\n    for sex in ['female', 'male']:\n        print('Pclass {} {}s: {}'.format(pclass, sex, age_pclass_sex[pclass][sex]))\nprint('All passengers: {}'.format(train['Age'].median()))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.159443Z","iopub.execute_input":"2021-12-11T19:29:53.159877Z","iopub.status.idle":"2021-12-11T19:29:53.178676Z","shell.execute_reply.started":"2021-12-11T19:29:53.159823Z","shell.execute_reply":"2021-12-11T19:29:53.177634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now take this information and use it to fill the null values for *Age* using the median of their class and sex combinations (possible since no null values for either *Pclass* or *Sex*).","metadata":{}},{"cell_type":"code","source":"# Filling the missing values in Age with the medians of Sex and Pclass groups\ncombined['Age'] = combined.groupby(['Pclass', 'Sex'])['Age'].apply(lambda x: x.fillna(x.median()))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.180047Z","iopub.execute_input":"2021-12-11T19:29:53.180375Z","iopub.status.idle":"2021-12-11T19:29:53.19296Z","shell.execute_reply.started":"2021-12-11T19:29:53.180334Z","shell.execute_reply":"2021-12-11T19:29:53.192172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Deck Feature:**\n\n*Cabin* is the feature with the most null values, a total of 879/1130 between the train and test sets.  This is too large of a proportion to impute, and the data would likely not be very valuable even if we did.  Instead, we will create a new feature, *Deck*, using the first letter of the *Cabin* values which indicates the deck where the cabin is.  We will denote all of the null values for *Cabin* with 'N' for *Deck*.","metadata":{}},{"cell_type":"code","source":"# Creating Deck column from the first letter of the Cabin column (N denotes a null entry)\ncombined['Deck'] = combined['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'N')\ntrain, test = divide_df(combined, TRAIN_LEN)\n\n# Create counts for all Deck, Pclass combinations (training data)\ndeck_class_count = train.groupby(['Deck', 'Pclass']).count().rename(columns={'Name': 'Count'})\ndeck_class_count = deck_class_count[['Count']]\nprint('Passenger counts for each Deck, Pclass combination (training data) where N deck denotes null values:', \n      deck_class_count, sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.194604Z","iopub.execute_input":"2021-12-11T19:29:53.195058Z","iopub.status.idle":"2021-12-11T19:29:53.213321Z","shell.execute_reply.started":"2021-12-11T19:29:53.195021Z","shell.execute_reply":"2021-12-11T19:29:53.212292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The previous cell's output provides some useful insight into how the decks are laid out.  However, since the number of cabins per deck is not constant, a proportion is more illustrative.  The following cell calculates the proportion of each passanger class by deck (training data) and then plots it.","metadata":{}},{"cell_type":"code","source":"# Calculate count and proportion of passenger 'Deck', 'Pclass' combinations\ndeck_class_count, deck_class_prop = get_deck_class_count(deck_class_count, True)\n\n# Plot Passenger class distribution by deck\ny_label = 'Passenger Class Proportion (%)'\ntitle = 'Passenger Class Distribution by Deck\\n for Titanic Final Voyage (Training Set)'\ndisplay_class_dist(deck_class_prop, y_label, title)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.214627Z","iopub.execute_input":"2021-12-11T19:29:53.214839Z","iopub.status.idle":"2021-12-11T19:29:53.47675Z","shell.execute_reply.started":"2021-12-11T19:29:53.214813Z","shell.execute_reply":"2021-12-11T19:29:53.47618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 'T' deck shown above is the boat deck (top deck) and only has 1 passenger.  For reference, decks 'A' through 'G' start just below the boat deck and run down towards the bottom of the ship.","metadata":{}},{"cell_type":"code","source":"display(combined[combined['Deck'] == 'T'])\nprint('Median fare for similar passengers (first class with no family onboard): {:.2f}'\n      .format(med_fare[1][0][0]))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.479354Z","iopub.execute_input":"2021-12-11T19:29:53.479695Z","iopub.status.idle":"2021-12-11T19:29:53.497478Z","shell.execute_reply.started":"2021-12-11T19:29:53.479666Z","shell.execute_reply":"2021-12-11T19:29:53.496511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will move them to 'A' deck, the closest physically to the boat deck which conveniently only has other first class passengers.  This will ideally lead to better predictions as we only have a samply size of one for 'T' deck.  We no longer need the *Cabin* feature either now that we have extracted the data we could from it.","metadata":{}},{"cell_type":"code","source":"# Passenger in the T deck is changed to A\ncombined = combine_df(train, test)\ni = combined[combined['Deck'] == 'T'].index\ncombined.loc[i, 'Deck'] = 'A'\n\ndeck_class_count.pop('T', None)\ndeck_class_count['A'][1] += 1\n\n# Drop the Cabin feature\nCOLS_TO_DROP.extend(['Cabin'])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.498765Z","iopub.execute_input":"2021-12-11T19:29:53.499022Z","iopub.status.idle":"2021-12-11T19:29:53.519684Z","shell.execute_reply.started":"2021-12-11T19:29:53.498984Z","shell.execute_reply":"2021-12-11T19:29:53.518561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Correlations:**\n\nNow that we have concluded dealing with the null values, we can investigate the correlations between our features to look for further insights into the dataset.  These insights will guide us in engineering other features as well as illuminate any possibly redundant features.","metadata":{}},{"cell_type":"code","source":"# Get training set correlations, focus on high correlations\ntrain, test = divide_df(combined, TRAIN_LEN)\ncorr_train = get_corr(train)\nhigh_corr_train = corr_train['Correlation Coefficient'] > 0.1\nprint('Training set correlations (coefficient > 0.1):')\ndisplay(corr_train[high_corr_train])\n\n# Get testing set correlations, focus on high correlations\ncorr_test = get_corr(test)\nhigh_corr_test = corr_test['Correlation Coefficient'] > 0.1\nprint('Testing set correlations (coefficient > 0.1):')\ndisplay(corr_test[high_corr_test])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.520901Z","iopub.execute_input":"2021-12-11T19:29:53.521184Z","iopub.status.idle":"2021-12-11T19:29:53.558013Z","shell.execute_reply.started":"2021-12-11T19:29:53.521146Z","shell.execute_reply":"2021-12-11T19:29:53.557234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating DeckClassSurvProp feature:**\n\nWhen investigating our initial assumptions, we saw that the survival rates for the various classes were quite disparate.  Later, when filling the null values for the *Cabin* feature, we saw that the various decks seemed to be organized in a way as to segregate the passenger classes.  These factors suggest that we may extract some predictive value from taking a closer look at the deck and class combinations, and their survival rates.","metadata":{}},{"cell_type":"code","source":"# Create count for number of passengers survived based on 'Deck', 'Pclass' combinations\ndeck_class_surv_count = train[['Deck', 'Pclass', 'Survived']]\ndeck_class_surv_count = deck_class_surv_count.groupby(['Deck', 'Pclass']).sum()\ndeck_class_surv_count, _ = get_deck_class_count(deck_class_surv_count, False)\n\n# Create proportion of passengers survived based on 'Deck', 'Pclass' combinations\ndeck_class_surv_prop = deck_class_surv_count.copy()\nfor deck in DECKS:\n    for pclass in deck_class_count[deck].keys():\n        try:\n            deck_class_surv_prop[deck][pclass] = round((deck_class_surv_count[deck][pclass] / \n                                                       deck_class_count[deck][pclass]), 2)\n        except ZeroDivisionError:\n            pass\n\n# Display information\nprint('Decimal percent of passengers survived for each \\'Deck\\', \\'Pclass\\' combination (training set)\\':')\ndisplay(deck_class_surv_prop)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.559301Z","iopub.execute_input":"2021-12-11T19:29:53.559595Z","iopub.status.idle":"2021-12-11T19:29:53.580403Z","shell.execute_reply.started":"2021-12-11T19:29:53.559562Z","shell.execute_reply":"2021-12-11T19:29:53.579768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing the training set and the combined set, we find that there are no *Deck*, *Pclass* combinations in the combined set that aren't already represented in the training set.  With that in mind, we can now safely apply these values as a new feature for both the train and test sets.  This feature is called *DeckClassSurvProp* and represents the proportion of passengers, with the same *Deck* and *Pclass* combination as the given passenger, who survived (only using data from training set to avoid leakage).","metadata":{}},{"cell_type":"code","source":"# Apply new feature to our data sets\ncombined = combine_df(train, test)\ncombined['DeckPclassSurvProp'] = combined.apply(lambda x: get_surv_prop(x['Deck'], x['Pclass']),axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.581545Z","iopub.execute_input":"2021-12-11T19:29:53.582525Z","iopub.status.idle":"2021-12-11T19:29:53.609608Z","shell.execute_reply.started":"2021-12-11T19:29:53.582485Z","shell.execute_reply":"2021-12-11T19:29:53.608803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create FamilySize Feature:**\n\nThe features *SibSp* and *Parch* are highly correlated and do not seem to independently provide valuable information.  As a result, we can improve our model by combining these two features into a new *FamilySize* feature which denotes the number of family members the passenger is travelling with, as well as themself.","metadata":{}},{"cell_type":"code","source":"# Create new feature 'FamilySize' from 'SibSp' and 'Parch'and drop those features\ncombined['FamilySize'] = combined['SibSp'] + combined['Parch'] + 1\nCOLS_TO_DROP.extend(['SibSp', 'Parch'])\ntrain, test = divide_df(combined, TRAIN_LEN)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.610901Z","iopub.execute_input":"2021-12-11T19:29:53.611556Z","iopub.status.idle":"2021-12-11T19:29:53.61911Z","shell.execute_reply.started":"2021-12-11T19:29:53.611507Z","shell.execute_reply":"2021-12-11T19:29:53.618451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we plot counts of *FamilySize* as well as checking the *Survived* distribution for each *FamilySize*.","metadata":{}},{"cell_type":"code","source":"# Plot relevant data\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2)\nsns.barplot(x=train['FamilySize'].value_counts().index, \n            y=train['FamilySize'].value_counts().values, ax=axs[0])\nsns.countplot(x='FamilySize', hue='Survived', data=train, ax=axs[1])\n\n# Tune plot\naxs[0].set_xlabel('Family Size')\naxs[0].set_ylabel('Count')\naxs[0].set_title('Family Size Value Counts', size=20)\naxs[1].set_xlabel('Family Size')\naxs[1].set_ylabel('Count')\naxs[1].set_title('Survived Counts by Family Size', size=20)\naxs[1].legend(['Died', 'Survived'], loc='upper right', prop={'size': 15})\nplt.subplots_adjust(top=0.4)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:53.620364Z","iopub.execute_input":"2021-12-11T19:29:53.620593Z","iopub.status.idle":"2021-12-11T19:29:54.074962Z","shell.execute_reply.started":"2021-12-11T19:29:53.620564Z","shell.execute_reply":"2021-12-11T19:29:54.074145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create FamilySizeCat Feature:**\n\nWe can see in the 'Survived Counts by Family Size' plot that there seems to be significant trends in survival based on *FamilySize*.  Those who are alone have comparably low chances of survival compared to small families of 2-4.  For families of size >=5, we see that people are again less likely to survive than die.  As a result, we will group the values of *FamilySize* together into the following categories and create a new feature called *FamilySizeCat*.\n\n- 1: Alone\n- 2-4: Small\n- 5-11: Large","metadata":{}},{"cell_type":"code","source":"# Create mapping of 'FamilySize' to its corresponding category and create new feature\nfamilysize_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Large', \n                  6: 'Large', 7: 'Large', 8: 'Large', 11: 'Large'}\ncombined = combine_df(train, test)\ncombined['FamilySizeCat'] = combined['FamilySize'].map(familysize_map)\ntrain, test = divide_df(combined, TRAIN_LEN)\n\n# Plot relevant data\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2)\nsns.barplot(x=train['FamilySizeCat'].value_counts().index, \n            y=train['FamilySizeCat'].value_counts().values, ax=axs[0])\nsns.countplot(x='FamilySizeCat', hue='Survived', data=train, ax=axs[1])\n\n# Tune plot\nxticks = ['Alone', 'Small', 'Large']\naxs[0].set_xlabel('Family Size Category')\naxs[0].set_ylabel('Count')\naxs[0].set_title('Family Size Category Value Counts', size=20)\naxs[1].set_xlabel('Family Size Category')\naxs[1].set_ylabel('Count')\naxs[1].set_title('Survived Counts by Family Size Category', size=20)\naxs[1].legend(['Died', 'Survived'], loc='upper right', prop={'size': 15})\nplt.subplots_adjust(top=0.4)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:54.077161Z","iopub.execute_input":"2021-12-11T19:29:54.077478Z","iopub.status.idle":"2021-12-11T19:29:54.39046Z","shell.execute_reply.started":"2021-12-11T19:29:54.07744Z","shell.execute_reply":"2021-12-11T19:29:54.389589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Title Feature:**\n\nThe *Name* feature appears to have titles ('Mr', 'Mrs', 'Master', etc.) for all passengers aboard.  We will take this part from *Name* and make it its own feature.","metadata":{}},{"cell_type":"code","source":"# Create new feature to extract passenger's title from the Name column\ncombined = combine_df(train, test)\ncombined['Title'] = combined['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\n\n# View the various values of these titles\nprint('Count of passenger titles aboard the Titanic:')\ncombined['Title'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:54.391906Z","iopub.execute_input":"2021-12-11T19:29:54.392227Z","iopub.status.idle":"2021-12-11T19:29:54.414608Z","shell.execute_reply.started":"2021-12-11T19:29:54.392185Z","shell.execute_reply":"2021-12-11T19:29:54.413742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that almost all the titles have too small of a sample size for the model to extract information from.  We will use the strategy that Peter Begle used in an article on [Medium](https://medium.com/i-like-big-data-and-i-cannot-lie/how-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9) to normalize these titles and give our model something to work with.","metadata":{}},{"cell_type":"code","source":"# Normalize the titles\nnormalized_titles = {\n    \"Capt\":         \"Officer\",\n    \"Col\":          \"Officer\",\n    \"Don\":          \"Royalty\",\n    \"Dona\":         \"Royalty\",\n    \"Dr\":           \"Officer\",\n    \"Jonkheer\":     \"Royalty\",\n    \"Lady\" :        \"Royalty\",\n    \"Major\":        \"Officer\",\n    \"Master\" :      \"Master\",\n    \"Miss\" :        \"Miss\",\n    \"Mlle\":         \"Miss\",\n    \"Mme\":          \"Mrs\",\n    \"Mr\" :          \"Mr\",\n    \"Mrs\" :         \"Mrs\",\n    \"Ms\":           \"Mrs\",\n    \"Rev\":          \"Officer\",\n    \"Sir\" :         \"Royalty\",\n    \"the Countess\": \"Royalty\"}\n\n# Map the current titles to the normalized titles\ncombined['Title'] = combined['Title'].map(normalized_titles)\n\n# View the new normalized values of these titles\nprint('Count of updated passenger titles aboard the Titanic:')\ncombined['Title'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:54.416068Z","iopub.execute_input":"2021-12-11T19:29:54.416479Z","iopub.status.idle":"2021-12-11T19:29:54.429573Z","shell.execute_reply.started":"2021-12-11T19:29:54.416436Z","shell.execute_reply":"2021-12-11T19:29:54.428933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that they are organized succinctly, we check to see the survival rates by title.","metadata":{}},{"cell_type":"code","source":"print('Decimal percentages for survival based on passenger title:')\ncombined[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\\\n                               .sort_values(by='Survived', kind=\"quicksort\", ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:54.430371Z","iopub.execute_input":"2021-12-11T19:29:54.430558Z","iopub.status.idle":"2021-12-11T19:29:54.459715Z","shell.execute_reply.started":"2021-12-11T19:29:54.430535Z","shell.execute_reply":"2021-12-11T19:29:54.458953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that title provides us with useful information as the likelihood of survival is drastically different between titles.  As a result, we will later use ordinal encoding to encapsulate the inherent hierarchy in likelihood of survival.","metadata":{}},{"cell_type":"markdown","source":"**Create Surname Feature:**\n\nAnother feature we can create is a *Surname* feature from the *Name* feature.  This may be useful as it seems likely that if one family member survived, others or perhaps all of the family would have survived.","metadata":{}},{"cell_type":"code","source":"combined['Surname'] = combined['Name'].apply(lambda x: str.split(x, \",\")[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:54.46079Z","iopub.execute_input":"2021-12-11T19:29:54.46098Z","iopub.status.idle":"2021-12-11T19:29:54.467437Z","shell.execute_reply.started":"2021-12-11T19:29:54.460957Z","shell.execute_reply":"2021-12-11T19:29:54.466756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, it is important to take *Surname* feature with a grain of salt as different families travelling onboard can have the same *Surname*.  We can see that with the 'Davies' *Surname*.  There appears to be some strange data for these individuals that is incompatible.  Namely the *Parch*, *SibSp*, and *Ticket* values.  ","metadata":{}},{"cell_type":"code","source":"display(combined.loc[combined['Surname'] == 'Davies'])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:54.468784Z","iopub.execute_input":"2021-12-11T19:29:54.468986Z","iopub.status.idle":"2021-12-11T19:29:54.497692Z","shell.execute_reply.started":"2021-12-11T19:29:54.468962Z","shell.execute_reply":"2021-12-11T19:29:54.496748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create FamilySurvival Feature:**\n\nDue to the issue with the *Surname* feature mentioned above, we will need to minimize the issue of grouping together passengers who are not travelling together.  This will be done by grouping passengers together based on *Surname*, *FamilySize*, and *Ticket*.  *Ticket* is included due to the observation that the *Ticket* is shared across all passengers travelling together.  We can see below from the largest family travelling on the Titanic.","metadata":{}},{"cell_type":"code","source":"combined.loc[combined['FamilySize'] == 11]","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:54.499031Z","iopub.execute_input":"2021-12-11T19:29:54.499262Z","iopub.status.idle":"2021-12-11T19:29:54.525914Z","shell.execute_reply.started":"2021-12-11T19:29:54.499236Z","shell.execute_reply":"2021-12-11T19:29:54.525294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given this fact, we will use it to group together families aboard the Titanic.  Note that the value for *DEFAULT_SURVIVAL* of 0.5 is just a placeholder and will be replaced during encoding.  The value is just to differentiate between families with known survivors and known deaths.  The idea here is that passengers in families with known survivors are more likely to survive.  Contrarily, passengers in families with known deaths are less likely to survive.","metadata":{}},{"cell_type":"code","source":"combined = group_survivors(combined, ['Surname', 'Fare', 'FamilySize'], 'FamilySurvival')\nprint('Count of passengers with family survival data: ', \n      combined.loc[combined['FamilySurvival']!=0.5].shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:54.52697Z","iopub.execute_input":"2021-12-11T19:29:54.52756Z","iopub.status.idle":"2021-12-11T19:29:54.853183Z","shell.execute_reply.started":"2021-12-11T19:29:54.527525Z","shell.execute_reply":"2021-12-11T19:29:54.852217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create GroupSurvival Feature:**\n\nSimilar to the reasoning above for the *FamilySurvival* feature, we will create a new feature *GroupSurvival* simply based on having the same *Ticket*.  There will be overlap between the two features, but there is still some usefulness to be extracted.  Passengers who are family would be likely to stick together and as a result, likely survive or perish together.  The sentiment can be extended to passengers who are travelling together but are not family (ie. friends, nannies, etc.).  These individuals may not be in the same cabin as would be true for families, so sticking together would not be as easy during the chaos of the sinking ship.","metadata":{}},{"cell_type":"code","source":"combined = group_survivors(combined, ['Ticket'], 'GroupSurvival')\nprint('Count of passenger with group survival data: ', \n      combined[combined['GroupSurvival']!=0.5].shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:54.854622Z","iopub.execute_input":"2021-12-11T19:29:54.855069Z","iopub.status.idle":"2021-12-11T19:29:55.217226Z","shell.execute_reply.started":"2021-12-11T19:29:54.85504Z","shell.execute_reply":"2021-12-11T19:29:55.216396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Discretize Continuous Features:**\n\nNow we can discretize our continuous features (*Age*, *Fare*) by placing the data into bins where each bin has an equal ($\\pm 1$) number of passengers.  This provides a better signal-to-noise ratio so long as the bins are not too large.  This binning also serves to encode our *Age* and *Fare* features as the *labels* parameter to *qcut* is provided.","metadata":{}},{"cell_type":"code","source":"# Binning 'Age' feature\nAGE_CUTS = 10\ntrain, test = divide_df(combined, TRAIN_LEN)\ntrain['Age'], age_bins = pd.qcut(train['Age'], AGE_CUTS, labels=list(range(AGE_CUTS)), retbins=True)\nage_bins[0] = -1\ntest['Age'] = pd.cut(test['Age'], labels=list(range(AGE_CUTS)), bins=age_bins)\n\n# Binning 'Fare' feature\nFARE_CUTS = 10\ntrain['Fare'], fare_bins = pd.qcut(train['Fare'], FARE_CUTS, labels=list(range(FARE_CUTS)), retbins=True)\nfare_bins[0] = -1\ntest['Fare'] = pd.cut(test['Fare'], labels=list(range(FARE_CUTS)), bins=fare_bins)\n\n# Return 'Age' and 'Fare' features back to integers from category\ncombined = combine_df(train, test)\ncombined[['Age', 'Fare']] = combined[['Age', 'Fare']].astype('int32')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:55.218343Z","iopub.execute_input":"2021-12-11T19:29:55.218558Z","iopub.status.idle":"2021-12-11T19:29:55.244221Z","shell.execute_reply.started":"2021-12-11T19:29:55.218532Z","shell.execute_reply":"2021-12-11T19:29:55.243498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we look at our binned groups for *Age*.  Keep in mind that the x-axis labels are the top end of the age range, so the count includes all values from the previous bar label to the current one ($[0, 16)$, $[16, 20)$, etc.).","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=train)\n\nplt.xlabel('Age (range upper boundaries)')\nplt.ylabel('Count')\nplt.xticks(ticks=list(range(10)), labels=age_bins[1:])\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:55.245396Z","iopub.execute_input":"2021-12-11T19:29:55.246053Z","iopub.status.idle":"2021-12-11T19:29:55.51521Z","shell.execute_reply.started":"2021-12-11T19:29:55.246012Z","shell.execute_reply":"2021-12-11T19:29:55.514316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that survival rates are quite different between the various age bins with only the $[0, 16)$ and $[34, 40)$ age groups with a survival rate $>50\\%$.  The same plot as above for *Age* is now provided for *Fare*.  Keep in mind again that the x-axis labels are the upper boundary for the age range bin ($[0, 7.55)$, $[7.55, 7.85)$, etc.).","metadata":{}},{"cell_type":"code","source":"# Round fares to cents\nfare_bins = [round(fare, 2) for fare in fare_bins]\n\n# Plot Fare bins\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=train)\n\n# Tune plot\nplt.xlabel('Fare (range upper boundaries)')\nplt.ylabel('Count')\nplt.xticks(ticks=list(range(10)), labels=fare_bins[1:])\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Fare'), size=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:55.516348Z","iopub.execute_input":"2021-12-11T19:29:55.516572Z","iopub.status.idle":"2021-12-11T19:29:55.795353Z","shell.execute_reply.started":"2021-12-11T19:29:55.516546Z","shell.execute_reply":"2021-12-11T19:29:55.79455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see a much clearer trend for *Fare* than we could for *Age*.  The survival rate consistently trends upwards as *Fare* increases (with the exception of the outlier $[27.00, 39.69)$ range).  \n\nThe number of bins for discretization of *Age* and *Fare* is not too large as the bins are able to uniquely capture the various behaviour of how these features affect *Survived*.","metadata":{}},{"cell_type":"markdown","source":"**Encode Features:**\n\nNow we encode our desired features.  We will begin using ordinal encoding for the *Deck* and *Title* features.","metadata":{}},{"cell_type":"code","source":"# Define the ordering of our encoding on our desired features\nord_cols = ['Deck', 'Title', 'FamilySurvival', 'GroupSurvival']\ndeck_cat = ['N', 'G', 'F', 'E', 'D', 'C', 'B', 'A']\ntitle_cat = ['Mr', 'Officer', 'Master', 'Royalty', 'Miss', 'Mrs']\nfamily_cat = group_cat = [0.0, 0.5, 1.0]\n\n# Encode features\nord_enc = OrdinalEncoder(categories=[deck_cat, title_cat, family_cat, group_cat])\ncombined[ord_cols] = ord_enc.fit_transform(combined[ord_cols])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:55.796566Z","iopub.execute_input":"2021-12-11T19:29:55.797442Z","iopub.status.idle":"2021-12-11T19:29:55.811767Z","shell.execute_reply.started":"2021-12-11T19:29:55.797396Z","shell.execute_reply":"2021-12-11T19:29:55.810728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we use one hot encoding on our remaining categorical variables.","metadata":{}},{"cell_type":"code","source":"# OneHotEncode our desired features\noh_cols = ['Embarked', 'FamilySizeCat', 'Sex']\ncombined = pd.get_dummies(data=combined, columns=oh_cols)\ntrain, test = divide_df(combined, TRAIN_LEN)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:55.812852Z","iopub.execute_input":"2021-12-11T19:29:55.813401Z","iopub.status.idle":"2021-12-11T19:29:55.826403Z","shell.execute_reply.started":"2021-12-11T19:29:55.813363Z","shell.execute_reply":"2021-12-11T19:29:55.82573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove Unnecessary Features:**\n\nFinally, we can remove any features that have become redundant, or simply not helpful for our purposes.","metadata":{}},{"cell_type":"code","source":"# Simplify dataset, but keep ids for submission\npassengerid_test = test.PassengerId\nCOLS_TO_DROP.extend(['PassengerId', 'Name', 'Ticket', 'Surname'])\ndrop_cols(COLS_TO_DROP)\n\n# View data after all preprocessing\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:55.827469Z","iopub.execute_input":"2021-12-11T19:29:55.8281Z","iopub.status.idle":"2021-12-11T19:29:55.862004Z","shell.execute_reply.started":"2021-12-11T19:29:55.828047Z","shell.execute_reply":"2021-12-11T19:29:55.861252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Model:**\n\nNow that we are finished with preprocessing the data, we can begin to initialize our model.  We begin by defining our training, validation, and testing features/targets.","metadata":{}},{"cell_type":"code","source":"# Explicitly split target from features\ny = train[['Survived']].copy()\ntrain.drop(columns='Survived', inplace=True)\n\n# Break off validation set from training data, separate target from features\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.8, random_state=RNG_SEED)\n\n# Rename for variable name consistency\nX_test = test.copy()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:55.863266Z","iopub.execute_input":"2021-12-11T19:29:55.863487Z","iopub.status.idle":"2021-12-11T19:29:55.87269Z","shell.execute_reply.started":"2021-12-11T19:29:55.86346Z","shell.execute_reply":"2021-12-11T19:29:55.871648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we define our model and use the best parameters we are able to find using *GricSearchCV* on our training data.  Change *WANT_HYPERPARAMETERS = TRUE* if the calculation is desired.  *WANT_HYPERPARAMETERS = False* will give best parameters from previous runs for the sake of time.","metadata":{}},{"cell_type":"code","source":"# Keep False to use best parameters from previous run\nWANT_HYPERPARAMETERS = False\n\nif WANT_HYPERPARAMETERS:\n    # Define parameters ranges to be tested\n    params = dict(max_depth = [n for n in range(3, 9)], \n                  min_samples_split = [n for n in range(2, 4)], \n                  min_samples_leaf = [n for n in range(2, 4)],\n                  n_estimators = [20, 40, 60, 80],)\n\n    # Define the model and find best parameters\n    model = GridSearchCV(RandomForestClassifier(random_state=RNG_SEED),\n                         params, cv=5, scoring='accuracy')\n    model.fit(X_train, y_train)\n\n    print(f'Best parameters {model_random_forest.best_params_}')\n    print(f'Mean cross-validated accuracy of the best parameters: {model.best_score_:.4f}')\n    \nelse:\n    model = RandomForestClassifier(n_estimators=50, max_depth=5, \n                                   min_samples_leaf=2, min_samples_split=2, \n                                   random_state=RNG_SEED)\n    model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:55.87382Z","iopub.execute_input":"2021-12-11T19:29:55.874386Z","iopub.status.idle":"2021-12-11T19:29:55.966177Z","shell.execute_reply.started":"2021-12-11T19:29:55.874352Z","shell.execute_reply":"2021-12-11T19:29:55.965282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the model is initialized and trained with the best parameters we could find, we use cross validation on different validation sets to get an accurate view of the model's performance on our training set.","metadata":{}},{"cell_type":"code","source":"# Processing of validation data, get predictions\npredictions_valid = model.predict(X_valid)\n\n# Evaluate the model using validation set\nscore = mean_absolute_error(y_valid, predictions_valid)\nprint('MAE for validation set prediction:', round(score, 4))\n\n# Multiply by -1 since sklearn calculates negative MAE\nfolds = 5\nscores = -1 * cross_val_score(model, X_valid, y_valid, cv=folds, scoring='neg_mean_absolute_error')\n\nprint('Average MAE score (across experiments using {} folds):'.format(folds))\nprint(round(scores.mean(), 4))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:55.967378Z","iopub.execute_input":"2021-12-11T19:29:55.967597Z","iopub.status.idle":"2021-12-11T19:29:56.368854Z","shell.execute_reply.started":"2021-12-11T19:29:55.967571Z","shell.execute_reply":"2021-12-11T19:29:56.368023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Importance:**\n\nNow that the model is trained and cross-validated, we check to see the feature importances.","metadata":{}},{"cell_type":"code","source":"# Sort values but keep indices\nsorted_idx = model.feature_importances_.argsort()\n\n# Plot data\nplt.figure(figsize=(15, 15))\nplt.barh(X_test.columns[sorted_idx], model.feature_importances_[sorted_idx])\nplt.xlabel('Feature Importance Coefficient')\nplt.ylabel('Feature')\nplt.title('Random Forest Classifier Feature Importances')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:56.369946Z","iopub.execute_input":"2021-12-11T19:29:56.370173Z","iopub.status.idle":"2021-12-11T19:29:56.705978Z","shell.execute_reply.started":"2021-12-11T19:29:56.370145Z","shell.execute_reply":"2021-12-11T19:29:56.705289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Submission:**\n\nFinally, we calculate and submit our final predictions.","metadata":{}},{"cell_type":"code","source":"# Processing of test data, get predictions\npredictions_test = model.predict(X_test)\npredictions_test = predictions_test.astype(int)\n\n# Save test predictions to file\noutput = pd.DataFrame({'PassengerId': passengerid_test,\n                       'Survived': predictions_test})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T19:29:56.707391Z","iopub.execute_input":"2021-12-11T19:29:56.707611Z","iopub.status.idle":"2021-12-11T19:29:56.72498Z","shell.execute_reply.started":"2021-12-11T19:29:56.707584Z","shell.execute_reply":"2021-12-11T19:29:56.724122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank you to [Gunes Etivan's Titanic - Advanced Feature Engineering Tutorial](http://https://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial/notebook) and [Manav Sehgal's Titanic Data Science Solutions](http://https://www.kaggle.com/startupsci/titanic-data-science-solutions/notebook) for their helpful ideas and overall contribution to this competition.","metadata":{}}]}